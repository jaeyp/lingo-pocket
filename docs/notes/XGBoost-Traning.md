* 시계열 데이터 학습용 인기 파이썬 라이브러리
시계열 데이터 학습을 위해 널리 사용되고 강력한 기능을 제공하는 파이썬 라이브러리는 다음과 같습니다. 이 라이브러리들은 데이터 탐색, 전처리, 모델링 및 예측 등 시계열 분석의 전 과정에 활용됩니다.

1. 전처리 및 기초 분석용

라이브러리,주요 용도,특징
- Pandas,데이터 조작 및 전처리,"시계열 인덱싱(DatetimeIndex), 리샘플링, 이동 평균 계산 등 시계열 데이터 구조화 및 관리에 필수적입니다."
- NumPy,기초 수치 연산,"Pandas의 기반이 되며, 빠르고 효율적인 배열 연산을 제공합니다."
- Statsmodels,통계적 모델링,"ARMA, ARIMA, SARIMA 등 전통적인 통계적 시계열 모델 구현에 강점을 가집니다. 시계열 분해, 단위근 검정 등 다양한 통계 테스트 도구도 제공합니다."

2. 머신러닝 및 딥러닝 기반 예측용
라이브러리,주요 용도,특징
- Scikit-learn (sklearn),전통적인 머신러닝 모델,"선형 회귀, 랜덤 포레스트, XGBoost 등 다양한 머신러닝 알고리즘을 사용하여 시계열 특성(lagged features)을 기반으로 예측 모델을 구축할 때 사용됩니다."
- TensorFlow / Keras,딥러닝 모델,"RNN (Recurrent Neural Network), LSTM (Long Short-Term Memory) , GRU 등 복잡한 패턴을 학습하고 장기 의존성을 모델링하는 딥러닝 기반 시계열 예측에 사용됩니다."
- PyTorch,딥러닝 모델,"TensorFlow와 마찬가지로 유연하고 동적인 그래프 구조를 기반으로 LSTM, Transformer와 같은 딥러닝 모델을 구축하는 데 활용됩니다."

3. 특화된 시계열 예측 라이브러리
라이브러리,주요 용도,특징
- Prophet,메타(Meta)에서 개발한 예측 라이브러리,"비선형적인 추세, 연간/주간 계절성 및 공휴일 효과 등을 자동으로 모델링하여 비전문가도 쉽게 고품질 예측을 할 수 있도록 설계되었습니다. 특히 명확한 계절성과 많은 관측치가 있는 비즈니스 데이터에 유용합니다."
- AutoGluon,자동화된 머신러닝 (AutoML),시계열 예측을 포함한 다양한 ML 작업에 대해 여러 모델을 자동으로 결합하여 최상의 성능을 내는 앙상블 모델을 구축해 줍니다.
- Darts,유연한 시계열 라이브러리,다양한 전통적/ML/DL 모델을 하나의 인터페이스로 통합하여 쉽게 모델을 테스트하고 비교할 수 있게 해주는 라이브러리입니다.

어떤 라이브러리를 선택할지는 데이터의 특성(선형성, 계절성, 복잡성)과 필요로 하는 모델의 복잡도(통계적 vs. 딥러닝)에 따라 달라집니다.

======================================================================================

[2025-12-02, 6:25:13 PM] 안낙호: 트레이닝을 해보면 느끼겠지만 노이즈가 엄청난 데이터라 다루기 힘들어
[2025-12-02, 6:26:18 PM] 안낙호: 데이터 준비도 티어 전략을 써야지 시간을 많이 줄여
[2025-12-02, 6:26:32 PM] Jaey: 티어전략이 뭐에요
[2025-12-02, 6:26:53 PM] Jaey: 데이터는 계속 csv로 저장해요?
[2025-12-02, 6:26:56 PM] 안낙호: 원본 Trade 데이터에서 1티어 데이터 만들고, 트레이닝 성격에 맞게 1티어 데이터에서 2티어 데이터 만들고
[2025-12-02, 6:27:29 PM] 안낙호: csv 같은거. 그거 때문에 나한테 먼저 듣고 해야 한다는거지
[2025-12-02, 6:27:47 PM] 안낙호: csv, sqlite3 이런거 쓰면 절대 안돼
[2025-12-02, 6:27:59 PM] 안낙호: 머신러닝에 쓰는거 따로 있어

💡 머신러닝 학습에 적합한 데이터 포맷
"머신러닝에 쓰는 거 따로 있어"는 일반적으로 대용량 데이터셋의 빠른 로딩(I/O)과 효율적인 메모리 사용을 위해 설계된 파일 포맷을 의미합니다.

특히 거래 데이터(Trade Data)와 같은 대용량 시계열 데이터를 반복적으로 학습해야 하는 환경에서는 CSV나 SQLite3보다 훨씬 효율적인 포맷이 필수적입니다.

안낙호 님이 언급했을 가능성이 가장 높은 포맷들은 다음과 같습니다.

1. 컬럼 기반 스토리지 (Columnar Storage)
시계열 및 분석 데이터 학습에 가장 널리 사용되며, Parquet이 대표적입니다.

포맷,특징 및 장점,왜 CSV보다 좋은가?
- Parquet,"- 컬럼 기반 압축 및 인코딩: 데이터를 행(Row) 단위가 아닌 열(Column) 단위로 저장하여 압축률이 매우 높고, 특정 컬럼만 읽을 때 속도가 빠릅니다.","CSV는 행 기반(Row-based)이어서 필요한 일부 컬럼만 로드하더라도 전체 행을 읽어야 하지만, Parquet은 필요한 컬럼만 빠르게 가져올 수 있습니다. 이는 대규모 머신러닝 피처(Feature) 데이터 로딩 시간을 획기적으로 줄여줍니다."
- Feather/Arrow,- 메모리 효율적인 데이터 교환: Python (Pandas)과 R 같은 다양한 언어/시스템 간에 데이터를 매우 빠르게 읽고 쓸 수 있는 포맷입니다.,데이터를 디스크에 저장하고 메모리로 로드할 때 직렬화/역직렬화 오버헤드가 적어 학습 데이터 로딩 속도에 최적화되어 있습니다.

2. 대용량 수치 데이터 포맷
복잡한 고차원 시계열 데이터를 구조화하여 저장할 때 유용합니다.

포맷,특징 및 장점,왜 CSV/SQLite3보다 좋은가?
- HDF5 (Hierarchical Data Format),"- 계층적 구조: 대용량의 배열(Array) 데이터를 계층적으로 저장하는 데 최적화되어 있습니다. NumPy 배열과 유사하게 데이터를 효율적으로 처리하며, 데이터셋에 메타데이터를 함께 저장할 수 있습니다.","특히 고주파(High-Frequency) 시계열 데이터나, 복잡한 다차원 피처 맵을 저장할 때 단일 파일 내에서 구조적 접근이 용이합니다."

3. CSV 및 SQLite3가 비효율적인 이유

포맷,비효율적인 이유
- CSV,- 데이터 타입 추론: 데이터를 로드할 때마다 텍스트를 읽고 각 컬럼의 데이터 타입을 추론해야 하므로 느립니다. (느린 I/O)
,"- 압축 비효율: 데이터 사이즈가 커서 디스크 공간을 많이 차지하며, 로드 시 더 많은 디스크 읽기 작업이 필요합니다."
- SQLite3,"- 단일 파일 기반 DB: 데이터베이스를 파일 형태로 저장하지만, 머신러닝 학습에 필요한 대규모 병렬 읽기(Parallel Read)나 빠른 수치 연산 로딩에 최적화되어 있지 않습니다."
,- 트랜잭션 오버헤드: 일반적인 DB의 트랜잭션 관리 오버헤드가 순수한 데이터 I/O 속도를 저해할 수 있습니다.


🔍 티어(Tier) 전략의 역할

안낙호 님이 언급한 **'티어 전략'**은 원본의 노이즈가 많고 방대한 데이터를 학습 목적에 맞게 단계별로 정제하고 구조화하는 과정입니다.

원본 Trade 데이터 (가장 크고 노이즈 多)

1티어 데이터 (원본에서 정제된 기본적인 데이터)

2티어 데이터 (학습 성격에 맞게 피처 엔지니어링이 적용된 최종 데이터)

이렇게 데이터의 크기와 용도를 분리하여 효율적인 Parquet/HDF5 포맷으로 저장하면, 학습 시 불필요한 데이터를 로드하지 않아도 되므로 훈련 시간(Training Time)을 대폭 절약할 수 있습니다.


======================================================================================

Q. 위에서 말한 시계열 주가데이터 트레이닝은 뭘로 시켜? 어떤 라이브러리를 임포트해야하지?

주가 시계열 데이터 트레이닝에 사용할 라이브러리는 목표하는 모델의 복잡도와 접근 방식에 따라 달라집니다.

일반적으로 주가 데이터 예측은 노이즈가 심하고 복잡한 비선형 관계를 다루기 때문에, 전통적인 통계 모델보다는 머신러닝(ML) 또는 딥러닝(DL) 접근법이 주로 사용됩니다.

다음은 주가 데이터 트레이닝에 필요한 주요 라이브러리와 가져와야 할 모듈입니다.

🚀 주가 시계열 데이터 트레이닝 핵심 라이브러리

1. 데이터 준비 및 기초 분석
시계열 데이터의 로드, 정제, 시각화에 필수적입니다.

라이브러리,가져올 모듈,주요 용도
- Pandas,import pandas as pd,"CSV, Parquet, HDF5 파일 로드, 날짜 인덱싱, 리샘플링, 이동 평균 등 피처 엔지니어링의 기본 도구."
- NumPy,import numpy as np,"배열 연산 및 행렬 조작, 특히 딥러닝 모델의 입력 형태(Shape)를 맞추는 데 사용."
- Matplotlib,import matplotlib.pyplot as plt,"주가 데이터의 추세, 변동성, 패턴을 시각화하여 분석."

2. 머신러닝 (ML) 기반 트레이닝
트레이딩 전략에 필요한 피처(가격 변동률, 기술 지표 등)를 추출하여 모델을 학습시킬 때 유용합니다.

라이브러리,가져올 모듈,주요 용도
Scikit-learn,from sklearn.model_selection import train_test_split,"데이터를 훈련/검증/테스트 세트로 분할하거나, 선형 회귀(LinearRegression), 서포트 벡터 머신(SVR) 등 기본적인 ML 모델 학습."
XGBoost / LightGBM,import xgboost as xgb 또는 import lightgbm as lgb,주가 예측에서 가장 좋은 성능을 보이는 경우가 많은 앙상블 기반 경사 하강 트리(Gradient Boosting Tree) 모델 학습.
Ta-Lib,(필요시 설치),"MACD, RSI, 볼린저 밴드 등 **기술적 분석 지표(Technical Indicators)**를 빠르고 쉽게 계산하여 ML 모델의 입력 피처로 활용."

3. 딥러닝 (DL) 기반 트레이닝
시계열의 복잡한 비선형 패턴이나 장기적인 의존성을 학습할 때 사용합니다. **"노이즈가 엄청난 데이터"**를 다룰 때 특히 유용할 수 있습니다.

라이브러리,가져올 모듈,주요 용도
TensorFlow / Keras,"import tensorflow as tf  from tensorflow.keras.models import Sequential  from tensorflow.keras.layers import LSTM, Dense","LSTM (Long Short-Term Memory) , GRU 등 시퀀스 데이터를 처리하고 장기 의존성(Long-Term Dependencies)을 포착할 수 있는 순환 신경망(RNN) 계열 모델 구축."
PyTorch,import torch  import torch.nn as nn,"유연한 연구 개발 환경이 필요할 때 사용하며, Transformer 구조나 복잡한 딥러닝 모델을 구성할 때 활용."

✅ 모델 선택에 대한 조언

안낙호 님이 **'노이즈가 엄청난 데이터'**라고 언급한 만큼, 주가 트레이딩 모델을 구축할 때 다음과 같은 순서를 고려해 보세요.

데이터 준비: Parquet/Feather 포맷을 사용하고 Pandas를 이용해 필요한 기간과 컬럼만 효율적으로 로드합니다.

ML 모델 시도: 초기에는 XGBoost나 LightGBM 같은 트리 기반 모델을 사용해 피처의 중요도를 파악하고, 예측 성능의 기본 벤치마크를 만듭니다.

DL 모델 적용: 노이즈와 복잡한 관계를 다루기 위해 LSTM 모델을 시도합니다. 이 모델은 시퀀스 데이터에서 패턴을 추출하는 데 강력합니다.

가장 중요한 것은 데이터를 어떻게 **'티어 전략'**에 맞춰 잘 가공(피처 엔지니어링)하여 모델에 입력하느냐 입니다.

======================================================================================

[2025-12-02, 6:30:44 PM] 안낙호: 그리고 결정적으로 중요한 거는 Candle 데이터를 쓰지 않더라고
[2025-12-02, 6:31:29 PM] Jaey: 아 재미나네 출근하고 싶다 ㅠ
[2025-12-02, 6:32:38 PM] 안낙호: 그리고 시계열 학습 할 때는 지표도 그냥 넣으면 안되고, 연관 지표들끼리 가공해서 넣거나 정규화 해서 넣어야 된다
[2025-12-02, 6:33:13 PM] Jaey: 데이터 학습용으로 가공하는 프로그램을 먼저 만들어야 겠네
[2025-12-02, 6:33:17 PM] 안낙호: 전체 종목 학습시켜도 안되고, 특히 신규상장코인은 학습이든 테스트든 무조건 빼야해
[2025-12-02, 6:33:24 PM] 안낙호: 어 그렇게 했어


안낙호 님의 최근 발언들은 주가/코인 시계열 데이터를 다룰 때 매우 실용적이고 중요한 함정(Pitfall)들을 지적하고 있습니다. 핵심 내용을 정리하고, 이에 대한 추가적인 설명을 드리겠습니다.

🧐 안낙호 님의 발언 요약 및 분석
1. "결정적으로 중요한 거는 Candle 데이터를 쓰지 않더라고"
내용,분석 및 조언
- Candle 데이터를 쓰지 않음,"주식이나 코인 트레이딩에서 흔히 보는 봉 차트(Candlestick) 데이터(OHLC: Open, High, Low, Close)만을 사용하지 않는다는 의미입니다.  대신, **원본 거래 데이터(Trade Data)**나 **호가창 데이터(Order Book Data)**와 같이 더 세밀하고 노이즈가 많은 고빈도 데이터를 사용하거나, Tick 데이터를 리샘플링하여 볼륨, 거래량 분포, 변동성 지표 등을 피처로 사용하는 것을 선호한다는 뜻일 수 있습니다."
- 핵심,단순한 OHLC 캔들보다 더 깊고 다양한 피처를 추출해야 한다는 점을 강조합니다.

2. "시계열 학습 할 때는 지표도 그냥 넣으면 안되고, 연관 지표들끼리 가공해서 넣거나 정규화 해서 넣어야 된다"
내용,분석 및 조언
- 지표 가공 및 정규화,"**기술 지표(RSI, MACD 등)**를 그대로 입력 변수로 사용하면 안 됩니다. 지표 간의 상호작용을 나타내는 새로운 지표를 만들거나(예: A 지표의 현재값 / B 지표의 이동평균), 모델 학습 전에 반드시 **정규화(Normalization) 또는 표준화(Standardization)**를 해야 합니다."
- 핵심,"스케일링(Scaling) 작업은 딥러닝/머신러닝 학습의 필수 전처리 단계이며, 모델의 성능과 수렴 속도에 치명적인 영향을 줍니다. (예: sklearn.preprocessing.MinMaxScaler, StandardScaler 사용)"

3. "전체 종목 학습시켜도 안되고, 특히 신규상장코인은 학습이든 테스트든 무조건 빼야해"
내용,분석 및 조언
- 신규 상장 코인 제외,신규 상장 종목은 충분한 거래 이력과 패턴이 쌓이지 않아 극심한 노이즈와 특이한 변동성을 보입니다. 이러한 데이터를 학습에 포함하면 모델이 일반적인 시장 패턴이 아닌 **특정 이벤트에 과적합(Overfitting)**될 위험이 높습니다.
- 전체 종목 학습 지양,"시장 전체를 하나의 모델로 학습시키기보다, **종목의 특성(시가총액, 섹터, 변동성 등)**에 따라 그룹을 나누어 학습시키는 **'티어 전략'**과 연관될 수 있습니다."
- 핵심,**데이터 선택(Selection)**과 **데이터 필터링(Filtering)**이 모델 성능만큼 중요합니다.

🛠️ 다음 단계: 데이터 가공 프로그램 (Feature Engineering Pipeline)
안낙호 님의 조언대로, 트레이닝을 시작하기 전에 **데이터 학습용 가공 프로그램(Feature Engineering Pipeline)**을 먼저 만드는 것이 핵심입니다.

이 파이프라인에는 다음과 같은 단계들이 포함되어야 합니다.

데이터 로드 및 필터링: 효율적인 포맷(Parquet)으로 로드하고, 신규 상장 종목 및 학습에 불필요한 데이터를 제외합니다.

Candle/Tick 데이터 가공: OHLC 데이터뿐만 아니라 거래량, 체결 강도, 변동성(Volatility) 등 고급 피처를 추출합니다.

지표 연산 및 가공: 기술 지표를 계산하고, 지표 간의 비율, 변화율 등을 새로운 피처로 생성합니다.

스케일링: 생성된 모든 피처에 대해 정규화/표준화를 적용합니다.

입력 형태 변환: LSTM 모델 등을 사용하려면 데이터를 시퀀스 형태 $(Samples, Timesteps, Features)$로 변환해야 합니다.

가장 먼저 어떤 데이터 포맷을 사용해 이 파이프라인을 구축할지 결정하고, Pandas와 Scikit-learn의 전처리 모듈을 중심으로 개발을 시작하는 것이 좋습니다.


======================================================================================

Row-Based Data vs Column-Based Data


======================================================================================

Q. 텐서플로우나 파이토치같은 프레임워크를 사용하지 않고 파이썬 라이브러리로 할수 있는 방법은 없어?

네, TensorFlow나 PyTorch 같은 딥러닝 프레임워크를 사용하지 않고도 시계열 주가 데이터 학습 및 예측을 수행할 수 있는 강력한 파이썬 라이브러리들이 있습니다.

이 방법들은 딥러닝에 비해 GPU 설정의 복잡성이 낮고, **모델 해석력(Interpretability)**이 높으며, 특히 금융 및 비즈니스 시계열 데이터에서 뛰어난 성능을 보여주는 경우가 많습니다.

🌳 비(非) 딥러닝 기반 시계열 학습 방법
* 경사 하강 트리 기반 모델 (Tree-based Gradient Boosting)

시계열 데이터의 피처 엔지니어링이 잘 되어 있다면, 딥러닝 모델보다 빠르고 정확하게 예측할 수 있는 경우가 많습니다.

라이브러리,모델 및 특징,주요 사용 목적
- XGBoost,"eXtreme Gradient Boosting의 약자로, 속도가 빠르고 메모리 효율이 높으며 우수한 일반화 성능을 보여줍니다.","시계열 피처(lagged features, 기술 지표)를 입력으로 받아 다음 시점의 가격, 수익률, 또는 방향성(상승/하락)을 예측하는 분류/회귀 문제에 적합합니다."
- LightGBM,Microsoft에서 개발했으며 XGBoost보다 더 빠른 학습 속도와 적은 메모리 사용량을 제공합니다.,대규모 데이터셋과 고빈도 트레이딩 데이터처럼 빠른 훈련 속도가 중요할 때 유용합니다.
- Scikit-learn (RandomForestRegressor),앙상블 기법의 일종인 랜덤 포레스트를 사용합니다. 모델 해석이 비교적 쉽고 과적합 방지에 강점을 가집니다.,초기 실험 단계나 피처 중요도를 확인하며 간단한 베이스라인 모델을 구축할 때 적합합니다.

💡 참고: 이 모델들은 시계열 자체를 시퀀스로 처리하는 것이 아니라, 특정 시점의 "상태"를 나타내는 피처 벡터를 입력으로 사용합니다. 따라서 피처 엔지니어링 (이동 평균, 지표 값, 래그(Lag) 값 등)이 성능에 결정적인 역할을 합니다.


🛠️ 학습을 위한 주요 Python 라이브러리 임포트
TensorFlow/PyTorch 없이 학습을 진행한다면, 주로 다음 라이브러리들을 사용하게 됩니다.

```python
import pandas as pd           # 데이터 로딩 및 전처리 (필수)
import numpy as np            # 수치 연산

# 머신러닝 모델
import xgboost as xgb         # XGBoost 사용
# import lightgbm as lgb      # LightGBM 사용

# 전처리 및 평가
from sklearn.model_selection import train_test_split, TimeSeriesSplit # 데이터 분할 및 교차 검증
from sklearn.preprocessing import StandardScaler # 정규화/표준화
from sklearn.metrics import mean_squared_error    # 모델 성능 평가 지표

# 기술 지표 계산 (선택 사항)
import ta                     # (Technical Analysis) 기술 지표 계산용 라이브러리
```


======================================================================================

Q. XGBoost을 사용하여 시계열 피처를 예측하는 예시를 보여줘

네, **XGBoost (eXtreme Gradient Boosting)**는 딥러닝 없이 시계열 예측을 수행하는 데 가장 강력하고 널리 사용되는 머신러닝 알고리즘입니다.

XGBoost는 순환 신경망(RNN)처럼 시퀀스를 직접 처리할 수는 없기 때문에, 시계열 데이터를 **"특징 벡터(Feature Vector)"**로 변환하여 모델에 입력해야 합니다. 이 과정을 피처 엔지니어링이라고 하며, 주로 지연(Lag) 피처와 **이동 통계(Rolling Features)**를 사용합니다.

다음은 주가 시계열 데이터를 가정하여 XGBoost를 사용해 다음 날 종가(Close Price)를 예측하는 예시 코드와 설명입니다.

📈 XGBoost를 사용한 시계열 예측 예시
이 예시는 가상의 주가 데이터(close)를 사용하며, 목표는 t 시점의 데이터를 기반으로 t+1 시점의 종가를 예측하는 것입니다.

1. 필수 라이브러리 임포트
```python
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
```

2. 가상 시계열 데이터 생성

실제 주가 데이터를 로드했다고 가정하고, 100일간의 가상 종가(Close) 데이터를 생성합니다.
```python
# 100일치 가상 데이터 생성
np.random.seed(42)
dates = pd.date_range(start='2024-01-01', periods=100, freq='D')
initial_price = 10000
# 누적 곱을 이용해 주가처럼 보이게 만듦 (노이즈 포함)
prices = initial_price * (1 + np.random.randn(100) * 0.01).cumprod()

df = pd.DataFrame({'Date': dates, 'Close': prices})
df.set_index('Date', inplace=True)

print("--- 원본 데이터 (일부) ---")
print(df.head())
```

3. 핵심: 시계열 피처 엔지니어링

XGBoost의 입력으로 사용할 지연 피처와 이동 통계 피처를 생성합니다.

피처 유형,설명,XGBoost에서의 역할
- 지연 피처 (Lagged Features),"t−1,t−2,… 시점의 종가 값입니다.",모델에게 과거의 패턴을 직접적으로 학습시킵니다.
- 이동 통계 (Rolling Features),"5일 이동평균(MA), 5일 표준편차(STD) 등 특정 기간의 통계량입니다.",모델에게 **추세(Trend)**나 변동성(Volatility) 같은 시장의 상태를 제공합니다.

```python   
# 1. 지연 피처 생성 (Lag Features)
# 이전 1일, 2일, 3일의 종가를 피처로 사용
for i in range(1, 4):
    df[f'Lag_{i}'] = df['Close'].shift(i)

# 2. 이동 통계 피처 생성 (Rolling Window Features)
# 5일 이동평균 (MA) 및 이동표준편차 (STD)
df['Rolling_Mean_5'] = df['Close'].rolling(window=5).mean().shift(1)
df['Rolling_Std_5'] = df['Close'].rolling(window=5).std().shift(1)

# 3. 목표 변수 (Target Variable) 생성
# 다음 날 종가를 예측 (Close 시점의 t+1 값을 목표로 설정)
df['Target'] = df['Close'].shift(-1)

# 피처 생성 시 초기에 NaN 값이 생기므로 제거 (가장 긴 Lag/Rolling 기간만큼 제거)
df.dropna(inplace=True)

print("\n--- 피처 엔지니어링된 데이터 (일부) ---")
print(df.head())
```

4. 데이터 분할 및 스케일링

XGBoost 학습을 위해 입력 데이터(X)와 목표 데이터(y)를 분리하고, 시계열 특성을 고려하여 훈련(Train) 데이터와 테스트(Test) 데이터를 분할합니다.

```python
# 입력 변수(피처)와 목표 변수 분리
X = df.drop(columns=['Target'])
y = df['Target']

# 피처 스케일링 (정규화/표준화)
# 지표도 정규화해야 한다는 조언에 따라 스케일러를 적용합니다.
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 시계열 특성을 고려하여 시간 순서대로 훈련/테스트 분할
# 예: 전체 데이터 중 80%를 훈련, 20%를 테스트
train_size = int(len(X) * 0.8)
X_train, X_test = X_scaled[:train_size], X_scaled[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

print(f"\n훈련 데이터 크기: {len(X_train)}")
print(f"테스트 데이터 크기: {len(X_test)}")
```

5. XGBoost 모델 훈련 및 예측

XGBRegressor를 사용하여 모델을 훈련시키고 테스트 데이터로 예측을 수행합니다.

```python
# XGBoost 회귀 모델 정의 및 훈련
# n_estimators: 부스팅 라운드 수 (트리 개수)
# max_depth: 트리의 최대 깊이
xgb_model = xgb.XGBRegressor(
    objective='reg:squarederror', 
    n_estimators=100, 
    learning_rate=0.05,
    max_depth=5, 
    random_state=42,
    n_jobs=-1 # 가능한 모든 CPU 코어 사용 (GPU는 아니지만 빠른 훈련 가능)
)

print("\n--- XGBoost 모델 훈련 시작 ---")
xgb_model.fit(X_train, y_train)
print("--- XGBoost 모델 훈련 완료 ---")

# 테스트 데이터 예측
predictions = xgb_model.predict(X_test)

# 성능 평가 (RMSE 사용)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f"\n테스트 데이터 RMSE (제곱근 평균 제곱 오차): {rmse:.2f}")

# 피처 중요도 확인 (어떤 피처가 예측에 가장 중요한지)
feature_importances = pd.Series(xgb_model.feature_importances_, index=X.columns).sort_values(ascending=False)
print("\n--- 피처 중요도 순위 ---")
print(feature_importances)
```

💡 XGBoost 학습의 결론

이 예시에서 알 수 있듯이, XGBoost는 Lag 피처와 Rolling 피처를 통해 과거 시계열 정보를 학습하여 다음 시점을 예측합니다. 피처 엔지니어링의 품질이 모델의 예측 성능을 결정하는 핵심 요소가 됩니다.

또한, 마지막에 출력된 **피처 중요도(Feature Importance)**를 통해 어떤 피처가 주가 예측에 가장 큰 기여를 했는지 직관적으로 파악할 수 있으며, 이는 딥러닝 모델의 블랙박스 문제에 비해 큰 장점입니다.